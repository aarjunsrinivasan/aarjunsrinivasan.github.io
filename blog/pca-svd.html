<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Is PCA Just SVD? Understanding the Relationship from ML to DL | Arjun Srinivasan</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,400;0,500;0,600;0,700;1,400&family=Source+Serif+4:ital,wght@0,400;0,600;0,700;1,400&display=swap" rel="stylesheet">
    
    <!-- MathJax for LaTeX rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
        };
    </script>
    
    <!-- Plotly.js for interactive visualizations -->
    <script src="https://cdn.plot.ly/plotly-2.26.0.min.js"></script>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    
    <style>
        :root {
            --bg: #fafaf9;
            --text: #292524;
            --text-muted: #57534e;
            --accent: #b45309;
            --accent-hover: #92400e;
            --border: #e7e5e4;
            --card-bg: #ffffff;
            --card-shadow: 0 1px 3px rgba(0, 0, 0, 0.06);
            --dark-bg: #1c1917;
            --dark-text: #fafaf9;
            --dark-text-muted: #a8a29e;
            --dark-accent: #d97706;
            --dark-accent-hover: #f59e0b;
            --dark-border: #44403c;
            --dark-card-bg: #292524;
            --dark-nav-bg: rgba(28, 25, 23, 0.96);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Source Sans 3', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.8;
            color: var(--text);
            background-color: var(--bg);
            transition: background-color 0.2s ease, color 0.2s ease;
        }

        [data-theme="dark"] body {
            background-color: var(--dark-bg);
            color: var(--dark-text);
        }

        /* Navigation */
        nav {
            position: fixed;
            top: 0;
            width: 100%;
            background: var(--card-bg);
            backdrop-filter: blur(10px);
            z-index: 1000;
            transition: background 0.2s ease, border-color 0.2s ease;
            border-bottom: 1px solid var(--border);
            box-shadow: var(--card-shadow);
        }

        [data-theme="dark"] nav {
            background: var(--dark-nav-bg);
            border-bottom: 1px solid var(--dark-border);
        }

        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1rem 2rem;
        }

        .logo {
            font-size: 1.4375rem;
            font-weight: 600;
            font-family: 'Source Serif 4', Georgia, serif;
            color: var(--text);
            text-decoration: none;
            letter-spacing: -0.02em;
        }

        [data-theme="dark"] .logo {
            color: var(--dark-text);
        }

        .nav-links {
            display: flex;
            list-style: none;
            gap: 2rem;
            align-items: center;
        }

        .nav-links a {
            text-decoration: none;
            color: var(--text);
            font-weight: 500;
            font-size: 0.9375rem;
            transition: color 0.2s ease;
        }

        [data-theme="dark"] .nav-links a {
            color: var(--dark-text);
        }

        .nav-links a:hover {
            color: var(--accent);
        }

        [data-theme="dark"] .nav-links a:hover {
            color: var(--dark-accent);
        }

        .theme-toggle {
            background: var(--card-bg);
            border: 1px solid var(--border);
            padding: 0.4rem;
            border-radius: 50%;
            cursor: pointer;
            font-size: 1rem;
            color: var(--accent);
            box-shadow: var(--card-shadow);
            transition: color 0.2s ease, border-color 0.2s ease;
            width: 42px;
            height: 42px;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        [data-theme="dark"] .theme-toggle {
            background: var(--dark-card-bg);
            color: var(--dark-accent);
            border-color: var(--dark-border);
        }

        .theme-toggle:hover {
            border-color: var(--accent);
            color: var(--accent-hover);
        }

        /* Blog Content */
        .blog-container {
            max-width: 900px;
            margin: 0 auto;
            padding: 120px 2rem 4rem;
        }

        .blog-header {
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--border);
        }

        [data-theme="dark"] .blog-header {
            border-bottom: 1px solid var(--dark-border);
        }

        .blog-title {
            font-size: 2.5rem;
            font-weight: 600;
            font-family: 'Source Serif 4', Georgia, serif;
            margin-bottom: 1rem;
            color: var(--text);
            line-height: 1.25;
            letter-spacing: -0.02em;
        }

        [data-theme="dark"] .blog-title {
            color: var(--dark-text);
        }

        .blog-meta {
            display: flex;
            gap: 1.5rem;
            color: var(--text-muted);
            font-size: 0.95rem;
            flex-wrap: wrap;
        }

        [data-theme="dark"] .blog-meta {
            color: var(--dark-text-muted);
        }

        .blog-content {
            font-size: 1.1rem;
            line-height: 1.9;
        }

        .blog-content h2 {
            font-size: 1.75rem;
            font-family: 'Source Serif 4', Georgia, serif;
            margin-top: 3rem;
            margin-bottom: 1rem;
            color: var(--text);
            font-weight: 600;
        }

        [data-theme="dark"] .blog-content h2 {
            color: var(--dark-text);
        }

        .blog-content h3 {
            font-size: 1.25rem;
            margin-top: 2rem;
            margin-bottom: 0.75rem;
            color: var(--text);
            font-weight: 600;
        }

        [data-theme="dark"] .blog-content h3 {
            color: var(--dark-text);
        }

        .blog-content p {
            margin-bottom: 1.5rem;
            text-align: justify;
        }

        .blog-content ul, .blog-content ol {
            margin-left: 2rem;
            margin-bottom: 1.5rem;
        }

        .blog-content li {
            margin-bottom: 0.5rem;
        }

        .blog-content code {
            background: var(--border);
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        [data-theme="dark"] .blog-content code {
            background: var(--dark-border);
        }

        .blog-content pre {
            background: var(--card-bg);
            border: 1px solid var(--border);
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
            border-left: 3px solid var(--accent);
            color: var(--text);
        }

        [data-theme="dark"] .blog-content pre {
            background: var(--dark-card-bg);
            border-color: var(--dark-border);
            border-left-color: var(--dark-accent);
            color: var(--dark-text);
        }

        .blog-content pre code {
            background: none;
            padding: 0;
            color: inherit;
        }

        .highlight-box {
            background: var(--card-bg);
            border: 1px solid var(--border);
            border-left: 4px solid var(--accent);
            padding: 1.5rem;
            margin: 2rem 0;
            border-radius: 8px;
        }

        [data-theme="dark"] .highlight-box {
            background: var(--dark-card-bg);
            border-color: var(--dark-border);
            border-left-color: var(--dark-accent);
        }

        .highlight-box h3 {
            color: var(--accent);
        }

        [data-theme="dark"] .highlight-box h3 {
            color: var(--dark-accent);
        }

        .math-display {
            margin: 2rem 0;
            text-align: center;
            overflow-x: auto;
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--accent);
            text-decoration: none;
            font-weight: 500;
            margin-bottom: 2rem;
            transition: color 0.2s ease;
        }

        .back-link:hover {
            color: var(--accent-hover);
        }

        [data-theme="dark"] .back-link {
            color: var(--dark-accent);
        }

        [data-theme="dark"] .back-link:hover {
            color: var(--dark-accent-hover);
        }

        /* MathJax styling */
        .MathJax {
            font-size: 1.1em !important;
        }

        .MathJax_Display {
            margin: 1.5rem 0 !important;
        }

        /* Figure and visualization styles */
        .figure-container {
            margin: 2.5rem 0;
            text-align: center;
        }

        .figure {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: var(--card-shadow);
            margin: 1rem 0;
        }

        .figure-caption {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
            font-style: italic;
        }

        [data-theme="dark"] .figure-caption {
            color: var(--dark-text-muted);
        }

        /* Code block enhancements */
        .code-block {
            position: relative;
            margin: 2rem 0;
        }

        .code-header {
            background: var(--accent);
            color: #fff;
            padding: 0.5rem 1rem;
            border-radius: 8px 8px 0 0;
            font-size: 0.85rem;
            font-weight: 600;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        [data-theme="dark"] .code-header {
            background: var(--dark-accent);
        }

        .code-block pre {
            margin-top: 0;
            border-radius: 0 0 8px 8px;
            background: #1e1e1e !important;
            color: #d4d4d4 !important;
        }

        [data-theme="dark"] .code-block pre {
            background: #0d1117 !important;
            color: #c9d1d9 !important;
        }

        .code-block pre code {
            color: inherit !important;
            background: none !important;
        }

        /* Interactive visualization container */
        .viz-container {
            background: var(--card-bg);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 2rem;
            margin: 2.5rem 0;
            text-align: center;
        }

        [data-theme="dark"] .viz-container {
            background: var(--dark-card-bg);
            border-color: var(--dark-border);
        }

        .viz-canvas {
            max-width: 100%;
            border-radius: 8px;
            margin: 1rem 0;
        }

        /* Comparison table */
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 2rem 0;
            box-shadow: var(--card-shadow);
            border-radius: 8px;
            overflow: hidden;
        }

        .comparison-table th {
            background: var(--accent);
            color: #fff;
            padding: 1rem;
            text-align: left;
            font-weight: 600;
        }

        [data-theme="dark"] .comparison-table th {
            background: var(--dark-accent);
        }

        .comparison-table td {
            padding: 1rem;
            border-bottom: 1px solid var(--border);
        }

        [data-theme="dark"] .comparison-table td {
            border-bottom-color: var(--dark-border);
        }

        .comparison-table tr:last-child td {
            border-bottom: none;
        }

        /* Side-by-side code comparison */
        .code-comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 1.5rem;
            margin: 2rem 0;
        }

        @media (max-width: 768px) {
            .code-comparison {
                grid-template-columns: 1fr;
            }
        }

        @media (max-width: 768px) {
            .blog-title {
                font-size: 2rem;
            }

            .blog-content h2 {
                font-size: 1.5rem;
            }

            .blog-content h3 {
                font-size: 1.25rem;
            }

            .blog-container {
                padding: 100px 1rem 2rem;
            }

            .nav-links {
                gap: 1rem;
                font-size: 0.9rem;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation -->
    <nav>
        <div class="nav-container">
            <a href="../index.html" class="logo">Arjun Srinivasan</a>
            <ul class="nav-links">
                <li><a href="../index.html#home">Home</a></li>
                <li><a href="../index.html#blogs">Blogs</a></li>
                <li><button class="theme-toggle" onclick="toggleTheme()" aria-label="Toggle theme">
                    <span class="theme-icon">üåì</span>
                </button></li>
            </ul>
        </div>
    </nav>

    <!-- Blog Content -->
    <div class="blog-container">
        <a href="../index.html#blogs" class="back-link">
            ‚Üê Back to Blogs
        </a>

        <div class="blog-header">
            <h1 class="blog-title">Is PCA Just SVD? A Geometric and Optimization View from ML to Deep Learning</h1>
            <div class="blog-meta">
                <span>üìÖ Published: January 2026</span>
                <span>‚è±Ô∏è Reading Time: 15 min</span>
                <span>üè∑Ô∏è Topics: Machine Learning, Deep Learning, Linear Algebra</span>
            </div>
        </div>

        <div class="blog-content">
            <p>
                Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) are two fundamental 
                techniques in machine learning and data science. The question "Is PCA just SVD?" is often asked, 
                and the answer reveals an important distinction about their nature and relationship.
            </p>

            <div class="highlight-box" style="border-left-width: 6px; padding: 2rem; margin: 2rem 0;">
                <h3 style="margin-top: 0; color: var(--accent);">The Answer</h3>
                <p style="font-size: 1.2rem; margin-bottom: 0.5rem;"><strong>Short answer: No.</strong></p>
                <p style="margin-bottom: 0;"><strong>Long answer:</strong> PCA is a statistical objective; SVD is a numerical linear algebra tool. 
                They meet‚Äîbut they are not the same thing.</p>
            </div>

            <p>
                In this post, we'll explore their mathematical foundations, understand why they coincide for centered data, 
                and see how this relationship plays out from classical machine learning to modern deep learning applications.
            </p>

            <h2>1. Singular Value Decomposition (SVD)</h2>

            <h3>1.1 Definition</h3>

            <p>
                Singular Value Decomposition is a fundamental matrix factorization technique from numerical linear algebra. 
                For any matrix $A \in \mathbb{R}^{m \times n}$, SVD decomposes it as:
            </p>

            <div class="math-display">
                $$A = U \Sigma V^\top$$
            </div>

            <p>where:</p>
            <ul>
                <li>$U \in \mathbb{R}^{m \times m}$ ‚Äî orthonormal columns (left singular vectors)</li>
                <li>$\Sigma \in \mathbb{R}^{m \times n}$ ‚Äî diagonal matrix with non-negative singular values $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r \geq 0$</li>
                <li>$V \in \mathbb{R}^{n \times n}$ ‚Äî orthonormal columns (right singular vectors)</li>
            </ul>

            <p>
                The singular values $\sigma_i$ are the square roots of the eigenvalues of both $A^T A$ and $A A^T$:
            </p>



            <p>
                SVD is a <strong>general matrix decomposition</strong>‚Äîit exists for any matrix (real or complex, square or rectangular) 
                which can be proved based on spectral theorem. It's a pure linear algebra tool with no statistical assumptions.
            </p>

            <h2>2. What Is PCA Really?</h2>

            <h3>2.1 PCA Is a Statistical Problem</h3>

            <p>
                Unlike SVD, Principal Component Analysis is fundamentally a <strong>statistical technique</strong>. 
                PCA is defined as:
            </p>

            <div class="highlight-box">
                <strong>PCA Objective:</strong> Find orthogonal directions that maximize the variance of projected data.
            </div>

            <p>
                This is a statistical optimization problem, not just a matrix factorization. The goal is to find 
                directions in the feature space along which the data exhibits maximum variability.
            </p>

            <h3>2.2 Data Matrix Setup</h3>

            <p>
                Let's set up the problem formally. Consider a data matrix:
            </p>

            <div class="math-display">
                $$X \in \mathbb{R}^{m \times n}$$
            </div>

            <p>where:</p>
            <ul>
                <li><strong>Rows = samples</strong> ($m$ data points)</li>
                <li><strong>Columns = features</strong> ($n$ features per sample)</li>
            </ul>

            <p>
                Each row $\mathbf{x}_i^T$ represents one data sample, and each column represents one feature across all samples.
            </p>

            <h3>2.3 Why Center the Data?</h3>

            <p>
                This is a crucial step that reveals the statistical nature of PCA. We center the data:
            </p>

            <div class="math-display">
                $$\tilde{X} = X - \boldsymbol{\mu}$$
            </div>

            <p>
                where $\boldsymbol{\mu} = \frac{1}{m}\sum_{i=1}^m \mathbf{x}_i$ is the sample mean vector.
            </p>

            <p>
                <strong>Why is centering necessary?</strong>
            </p>
            <ul>
                <li><strong>PCA measures variance:</strong> Variance is a measure of spread around the mean. 
                    By definition, variance is computed relative to the mean.</li>
                <li><strong>Variance is defined relative to the mean:</strong> For a random variable, 
                    $\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]$. Without centering, we're not measuring 
                    true variance.</li>
                <li><strong>Without centering ‚Üí first component points to the mean:</strong> If we don't center, 
                    the first principal component will be biased toward the direction of the mean vector, not the 
                    direction of maximum variance. This ensures PCA captures variance, not mean shift.</li>
            </ul>

            <p>
                More formally, let's derive why centering is necessary. Consider we want to find a direction 
                $\mathbf{w}$ (a unit vector) along which to project our data to maximize variance. Even if we start 
                with uncentered data, when we compute variance properly, we must subtract the mean. The variance 
                of the projections $z_i = \mathbf{x}_i^T \mathbf{w}$ along direction $\mathbf{w}$ is:
            </p>

            <div class="math-display">
                $$\text{Var}(z) = \frac{1}{m-1} \sum_{i=1}^m (z_i - \bar{z})^2 = \frac{1}{m-1} \sum_{i=1}^m (\mathbf{x}_i^T \mathbf{w} - \boldsymbol{\mu}^T \mathbf{w})^2$$
            </div>

            <p>
                Notice that even though we started with uncentered data $\mathbf{x}_i$, the variance formula 
                automatically subtracts the mean of the projections ($\bar{z} = \boldsymbol{\mu}^T \mathbf{w}$). 
                This can be rewritten as:
            </p>

            <div class="math-display">
                $$\text{Var}(z) = \frac{1}{m-1} \sum_{i=1}^m [(\mathbf{x}_i - \boldsymbol{\mu})^T \mathbf{w}]^2 = \mathbf{w}^T \left[\frac{1}{m-1} \sum_{i=1}^m (\mathbf{x}_i - \boldsymbol{\mu})(\mathbf{x}_i - \boldsymbol{\mu})^T\right] \mathbf{w} = \mathbf{w}^T \Sigma_x \mathbf{w}$$
            </div>

            <p>
                This shows that the variance along direction $\mathbf{w}$ depends on the covariance matrix 
                $\Sigma_x$ of <strong>centered</strong> data $(\mathbf{x}_i - \boldsymbol{\mu})$. The key insight 
                is that variance is always computed relative to the mean‚Äîthis is built into the definition of variance. 
                Therefore, to maximize variance along direction $\mathbf{w}$, we must work with centered data. 
                Without centering, we'd be optimizing a different objective that mixes variance with the mean location, 
                and the first principal component would point toward the mean rather than the direction of maximum variance.
            </p>

            <!-- <div class="figure-container">
                <div class="viz-container">
                    <div id="centering-viz" style="width: 100%; height: 400px;"></div>
                    <p class="figure-caption">Figure 1: Effect of centering on PCA. Left: Uncentered data - first PC points toward mean. Right: Centered data - first PC captures maximum variance.</p>
                </div>
            </div> -->

            <h2>3. Objective: What Are We Maximizing?</h2>

            <h3>3.1 Projection Viewpoint (Key Intuition)</h3>

            <p>
                The key intuition behind PCA comes from the projection viewpoint. We choose a unit direction 
                $\mathbf{w} \in \mathbb{R}^n$ (where $\|\mathbf{w}\| = 1$) and project all data points onto it:
            </p>

            <div class="math-display">
                $$z_i = \mathbf{x}_i^\top \mathbf{w}$$
            </div>

            <p>
                This gives us a 1D representation of the data. Each high-dimensional point $\mathbf{x}_i$ is 
                mapped to a scalar $z_i$ along the direction $\mathbf{w}$.
            </p>

            <div class="figure-container">
                <div class="viz-container">
                    <div id="projection-viz" style="width: 100%; height: 450px;"></div>
                    <p class="figure-caption">Figure 2: Projection viewpoint. Data points are projected onto a direction vector $\mathbf{w}$. PCA finds the direction that maximizes the variance of these projections.</p>
                </div>
            </div>

            <p>
                <strong>PCA asks:</strong> Along which direction does the projected data have maximum variance?
            </p>

            <p>
                This is the core statistical question. We want to find the direction that preserves the most 
                information about the data's variability.
            </p>

            <h3>3.2 Variance of the Projection</h3>

            <p>
                For centered data $\tilde{X}$, the variance of the projected data is:
            </p>

            <div class="math-display">
                $$\text{Var}(\tilde{X}\mathbf{w}) = \frac{1}{m} \sum_{i=1}^{m} (\tilde{\mathbf{x}}_i^\top \mathbf{w})^2$$
            </div>

            <p>
                Note: For centered data, the mean of projections is zero, so variance simplifies to the mean of squared projections. 
                This is because $\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$, and for centered data $\mathbb{E}[X] = 0$, 
                so $\text{Var}(X) = \mathbb{E}[X^2]$.
            </p>

            <p>
                Rewriting in matrix form:
            </p>

            <div class="math-display">
                $$\text{Var}(\tilde{X}\mathbf{w}) = \frac{1}{m} \sum_{i=1}^{m} (\tilde{\mathbf{x}}_i^\top \mathbf{w})^2 = \frac{1}{m} \mathbf{w}^\top \tilde{X}^\top \tilde{X} \mathbf{w}$$
            </div>

            <p>
                Define the sample covariance matrix:
            </p>

            <div class="math-display">
                $$\Sigma_x = \frac{1}{m} \tilde{X}^\top \tilde{X}$$
            </div>

            <p>
                So the objective becomes:
            </p>

            <div class="math-display">
                $$\boxed{ \max_{\mathbf{w}} \quad \mathbf{w}^\top \Sigma_x \mathbf{w} }$$
            </div>

            <h3>3.3 Constraint: Why Unit Norm?</h3>

            <p>
                Without a constraint, the objective is meaningless. We could simply scale $\mathbf{w}$ to make 
                the variance arbitrarily large:
            </p>

            <div class="math-display">
                $$\mathbf{w}^\top \Sigma_x \mathbf{w} \rightarrow \infty \quad \text{by scaling } \mathbf{w}$$
            </div>

            <p>
                So we impose the constraint:
            </p>

            <div class="math-display">
                $$\boxed{ \mathbf{w}^\top \mathbf{w} = 1 }$$
            </div>

            <p>
                <strong>Geometric meaning:</strong> We are choosing a <em>direction</em>, not a magnitude. 
                The constraint ensures we're optimizing over the unit sphere, which makes the problem well-defined.
            </p>

            <h3>3.4 Full Optimization Problem</h3>

            <p>
                Combining the objective and constraint, we have:
            </p>

            <div class="math-display">
                $$\boxed{ \max_{\mathbf{w}} \quad \mathbf{w}^\top \Sigma_x \mathbf{w} \quad \text{subject to} \quad \mathbf{w}^\top \mathbf{w} = 1 }$$
            </div>



            <h2>4. Solution via Lagrange Multipliers</h2>

            <p>
                We solve this constrained optimization problem using the method of Lagrange multipliers.
            </p>

            <h3>4.1 Step 1: Construct the Lagrangian</h3>

            <p>
                The Lagrangian function is:
            </p>

            <div class="math-display">
                $$\mathcal{L}(\mathbf{w}, \lambda) = \mathbf{w}^\top \Sigma_x \mathbf{w} - \lambda(\mathbf{w}^\top \mathbf{w} - 1)$$
            </div>

            <p>
                where $\lambda$ is the Lagrange multiplier associated with the constraint $\mathbf{w}^\top \mathbf{w} = 1$.
            </p>

            <h3>4.2 Step 2: Take Gradient with Respect to $\mathbf{w}$</h3>

            <p>
                Taking the gradient of the Lagrangian with respect to $\mathbf{w}$:
            </p>

            <div class="math-display">
                $$\nabla_{\mathbf{w}} \mathcal{L} = 2\Sigma_x \mathbf{w} - 2\lambda \mathbf{w}$$
            </div>

            <p>
                Setting the gradient to zero:
            </p>

            <div class="math-display">
                $$2\Sigma_x \mathbf{w} - 2\lambda \mathbf{w} = 0$$
            </div>

            <p>
                Dividing by 2 and rearranging:
            </p>

            <div class="math-display">
                $$\boxed{ \Sigma_x \mathbf{w} = \lambda \mathbf{w} }$$
            </div>

            <div class="highlight-box">
                <strong>üîë Key Result:</strong> The PCA directions are eigenvectors of the covariance matrix $\Sigma_x$. 
                The Lagrange multiplier $\lambda$ is the corresponding eigenvalue.
            </div>

            <h3>4.3 Step 3: Which Eigenvector?</h3>

            <p>
                To determine which eigenvector gives the maximum variance, substitute back into the objective. 
                From the eigenvalue equation:
            </p>

            <div class="math-display">
                $$\mathbf{w}^\top \Sigma_x \mathbf{w} = \mathbf{w}^\top (\lambda \mathbf{w}) = \lambda \mathbf{w}^\top \mathbf{w} = \lambda$$
            </div>

            <p>
                So:
            </p>

            <div class="highlight-box">
                <strong>Maximizing variance ‚áî maximizing eigenvalue</strong>
            </div>

            <p>
                Therefore:
            </p>
            <ul>
                <li><strong>First principal component</strong> = eigenvector with largest eigenvalue</li>
                <li><strong>Second principal component</strong> = eigenvector with second largest eigenvalue (orthogonal to the first)</li>
                <li><strong>Remaining components</strong> = next largest eigenvalues (all mutually orthogonal)</li>
            </ul>

            <h2>5. Geometric Interpretation (Very Important)</h2>

            <p>
                Understanding what PCA is doing geometrically provides crucial intuition:
            </p>

            <h3>5.1 What PCA Is Geometrically Doing</h3>

            <p>
                Consider a data cloud in $\mathbb{R}^n$. PCA finds orthogonal axes that:
            </p>
            <ul>
                <li><strong>Maximize spread:</strong> The first axis aligns with the direction of maximum variance</li>
                <li><strong>Minimize reconstruction error:</strong> When we project data onto these axes and reconstruct, 
                    we minimize the squared error</li>
            </ul>

            <p>
                This is equivalent to:
            </p>
            <ul>
                <li><strong>Rotating the coordinate system:</strong> We're finding a new orthonormal basis for the data space</li>
                <li><strong>Aligning axes with directions of maximal variance:</strong> The new coordinate axes point 
                    along the "principal directions" of the data</li>
            </ul>

            <p>
                The geometric interpretation connects the statistical objective (variance maximization) with the 
                linear algebra solution (eigenvalue decomposition).
            </p>

            <h2>6. The Connection: When Do PCA and SVD Meet?</h2>

            <p>
                Now we can understand when and why PCA and SVD coincide:
            </p>

            <p>
                For <strong>centered data</strong> $\tilde{X}$, the covariance matrix is:
            </p>

            <div class="math-display">
                $$\Sigma_x = \frac{1}{m} \tilde{X}^\top \tilde{X}$$
            </div>

            <p>
                If we apply SVD to the centered data matrix:
            </p>

            <div class="math-display">
                $$\tilde{X} = U \Sigma V^\top$$
            </div>

            <p>
                where $U \in \mathbb{R}^{m \times m}$ has orthonormal columns, $\Sigma \in \mathbb{R}^{m \times n}$ is diagonal 
                with singular values, and $V \in \mathbb{R}^{n \times n}$ has orthonormal columns.
            </p>

            <p>
                Now, let's compute $\tilde{X}^\top \tilde{X}$ using the SVD decomposition:
            </p>

            <div class="math-display">
                $$\tilde{X}^\top \tilde{X} = (U \Sigma V^\top)^\top (U \Sigma V^\top)$$
            </div>

            <p>
                Using the property $(AB)^\top = B^\top A^\top$, we get:
            </p>

            <div class="math-display">
                $$\tilde{X}^\top \tilde{X} = (V \Sigma^\top U^\top) (U \Sigma V^\top)$$
            </div>

            <p>
                Since $U$ has orthonormal columns, $U^\top U = I_m$ (the $m \times m$ identity matrix). Also, 
                $\Sigma^\top \Sigma = \Sigma^2$ (since $\Sigma$ is diagonal). Therefore:
            </p>

            <div class="math-display">
                $$\tilde{X}^\top \tilde{X} = V \Sigma^\top U^\top U \Sigma V^\top = V \Sigma^\top \Sigma V^\top = V \Sigma^2 V^\top$$
            </div>

            <p>
                where $\Sigma^2$ is a diagonal matrix with entries $\sigma_i^2$ (the squares of singular values).
            </p>

            <p>
                Now, the covariance matrix becomes:
            </p>

            <div class="math-display">
                $$\Sigma_x = \frac{1}{m} \tilde{X}^\top \tilde{X} = \frac{1}{m} V \Sigma^2 V^\top$$
            </div>

            <p>
                <strong>Key observation:</strong> This is an eigendecomposition!
            </p>

            <p>
                Multiplying both sides by $V$ from the right and using $V V^\top = I_n$:
            </p>

            <div class="math-display">
                $$\Sigma_x V = V \left(\frac{1}{m} \Sigma^2\right)$$
            </div>

            <p>
                This shows that $\Sigma_x V = V \Lambda$, where $\Lambda = \frac{1}{m} \Sigma^2$ is a diagonal matrix. 
                This is exactly the eigenvalue equation! Let's visualize what this means.
            </p>

            <p>
                The matrix $V$ contains the eigenvectors as <strong>columns</strong>:
            </p>

            <div class="math-display">
                $$V = \begin{bmatrix} \mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n \end{bmatrix} = \begin{bmatrix}
                v_{11} & v_{12} & \cdots & v_{1n} \\
                v_{21} & v_{22} & \cdots & v_{2n} \\
                \vdots & \vdots & \ddots & \vdots \\
                v_{n1} & v_{n2} & \cdots & v_{nn}
                \end{bmatrix}$$
            </div>

            <p>
                And $\Lambda$ is a diagonal matrix of eigenvalues:
            </p>

            <div class="math-display">
                $$\Lambda = \frac{1}{m} \Sigma^2 = \begin{bmatrix}
                \lambda_1 & 0 & \cdots & 0 \\
                0 & \lambda_2 & \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & \lambda_n
                \end{bmatrix} = \begin{bmatrix}
                \frac{\sigma_1^2}{m} & 0 & \cdots & 0 \\
                0 & \frac{\sigma_2^2}{m} & \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & \frac{\sigma_n^2}{m}
                \end{bmatrix}$$
            </div>

            <p>
                The eigenvalue equation $\Sigma_x V = V \Lambda$ means that each column $\mathbf{v}_i$ of $V$ satisfies:
            </p>

            <div class="math-display">
                $$\Sigma_x \mathbf{v}_i = \lambda_i \mathbf{v}_i$$
            </div>

            <p>
                Therefore:
            </p>
            <ul>
                <li>The <strong>columns of $V$</strong> (right singular vectors $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n$) 
                    are the <strong>eigenvectors of $\Sigma_x$</strong>, which are exactly the <strong>principal components</strong></li>
                <li>The <strong>eigenvalues</strong> are $\lambda_i = \frac{\sigma_i^2}{m}$, where $\sigma_i$ are the singular values</li>
                <li>The first principal component is $\mathbf{v}_1$ (corresponding to the largest eigenvalue $\lambda_1$)</li>
            </ul>

            <h3>6.1 Projecting Data onto Principal Directions</h3>

            <p>
                Now that we have found the principal directions (the columns of $V$), the next step is to project our 
                data onto these directions. This is where the left singular vectors $U$ come into play.
            </p>

            <p>
                For a data point $\tilde{\mathbf{x}}_i$ (a row of $\tilde{X}$) and principal component $\mathbf{v}_j$ 
                (a column of $V$), the projection is:
            </p>

            <div class="math-display">
                $$z_{ij} = \tilde{\mathbf{x}}_i^\top \mathbf{v}_j$$
            </div>

            <p>
                This gives the coordinate of data point $i$ along principal component $j$. When we project <strong>all</strong> 
                data points onto principal component $\mathbf{v}_j$, we get a vector containing the projections of all 
                $m$ data points:
            </p>

            <div class="math-display">
                $$\tilde{X} \mathbf{v}_j = \begin{bmatrix} \tilde{\mathbf{x}}_1^\top \\ \tilde{\mathbf{x}}_2^\top \\ \vdots \\ \tilde{\mathbf{x}}_m^\top \end{bmatrix} \mathbf{v}_j = \begin{bmatrix} \tilde{\mathbf{x}}_1^\top \mathbf{v}_j \\ \tilde{\mathbf{x}}_2^\top \mathbf{v}_j \\ \vdots \\ \tilde{\mathbf{x}}_m^\top \mathbf{v}_j \end{bmatrix}$$
            </div>

            <p>
                This vector $\tilde{X} \mathbf{v}_j \in \mathbb{R}^m$ contains the projections of all $m$ data points 
                onto the $j$-th principal component. Now, let's see how this relates to the SVD and the matrix $U$.
            </p>

            <h3>6.2 What Are the Left Singular Vectors $U$?</h3>

            <p>
                From the SVD decomposition $\tilde{X} = U \Sigma V^\top$, we can express the projection $\tilde{X} \mathbf{v}_j$ as:
            </p>

            <p>
                Now, from the SVD $\tilde{X} = U \Sigma V^\top$, we can write:
            </p>

            <div class="math-display">
                $$\tilde{X} \mathbf{v}_j = U \Sigma V^\top \mathbf{v}_j$$
            </div>

            <p>
                Since $V$ has orthonormal columns, $V^\top \mathbf{v}_j$ is the $j$-th standard basis vector $\mathbf{e}_j$ 
                (a vector with 1 in position $j$ and 0 elsewhere). Therefore:
            </p>

            <div class="math-display">
                $$\tilde{X} \mathbf{v}_j = U \Sigma \mathbf{e}_j = U \begin{bmatrix} 0 \\ \vdots \\ \sigma_j \\ \vdots \\ 0 \end{bmatrix} = \sigma_j \mathbf{u}_j$$
            </div>

            <p>
                where $\mathbf{u}_j$ is the $j$-th column of $U$. This shows that:
            </p>

            <div class="math-display">
                $$\tilde{X} \mathbf{v}_j = \sigma_j \mathbf{u}_j$$
            </div>

            <p>
                In other words, <strong>projecting all data points onto principal component $\mathbf{v}_j$ gives $\sigma_j \mathbf{u}_j$</strong>, 
                where $\mathbf{u}_j$ is the $j$-th left singular vector (normalized projection vector).
            </p>

            <p>
                More generally, when we project all data onto all principal components, we get:
            </p>

            <div class="math-display">
                $$\tilde{X} V = U \Sigma V^\top V = U \Sigma$$
            </div>

            <p>
                since $V^\top V = I_n$. The matrix $\tilde{X} V$ contains all projections: the $(i,j)$-th element is 
                the projection of data point $i$ onto principal component $j$. This equals $U \Sigma$, which means:
            </p>

            <div class="math-display">
                $$U \Sigma = \begin{bmatrix} \mathbf{u}_1 & \mathbf{u}_2 & \cdots & \mathbf{u}_m \end{bmatrix} \begin{bmatrix}
                \sigma_1 & 0 & \cdots & 0 \\
                0 & \sigma_2 & \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & \sigma_r
                \end{bmatrix} = \begin{bmatrix}
                \sigma_1 \mathbf{u}_1 & \sigma_2 \mathbf{u}_2 & \cdots & \sigma_r \mathbf{u}_r
                \end{bmatrix}$$
            </div>

            <p>
                Visualizing $U$ as columns:
            </p>

            <div class="math-display">
                $$U = \begin{bmatrix} \mathbf{u}_1 & \mathbf{u}_2 & \cdots & \mathbf{u}_m \end{bmatrix} = \begin{bmatrix}
                u_{11} & u_{12} & \cdots & u_{1m} \\
                u_{21} & u_{22} & \cdots & u_{2m} \\
                \vdots & \vdots & \ddots & \vdots \\
                u_{m1} & u_{m2} & \cdots & u_{mm}
                \end{bmatrix}$$
            </div>

            <p>
                We can also derive the relationship for $U$ by computing $\tilde{X} \tilde{X}^\top$:
            </p>

            <div class="math-display">
                $$\tilde{X} \tilde{X}^\top = (U \Sigma V^\top) (U \Sigma V^\top)^\top = U \Sigma V^\top V \Sigma^\top U^\top = U \Sigma \Sigma^\top U^\top = U \Sigma^2 U^\top$$
            </div>

            <p>
                where we used $V^\top V = I_n$ and $\Sigma \Sigma^\top = \Sigma^2$ (since $\Sigma$ is diagonal). 
                This shows:
            </p>

            <div class="math-display">
                $$\tilde{X} \tilde{X}^\top = U \Lambda' U^\top$$
            </div>

            <p>
                where $\Lambda' = \Sigma^2$ is a diagonal matrix. This is also an eigendecomposition, so:
            </p>

            <div class="math-display">
                $$(\tilde{X} \tilde{X}^\top) \mathbf{u}_j = \sigma_j^2 \mathbf{u}_j$$
            </div>

            <p>
                <strong>Summary of what $U$ represents:</strong>
            </p>
            <ul>
                <li>The <strong>columns of $U$</strong> (left singular vectors) are the <strong>eigenvectors of $\tilde{X} \tilde{X}^\top$</strong></li>
                <li>The <strong>$j$-th column $\mathbf{u}_j$</strong> is the <strong>normalized projection vector</strong>: when we project 
                    all data points onto principal component $\mathbf{v}_j$, we get $\sigma_j \mathbf{u}_j$</li>
                <li>The <strong>$i$-th row of $U$</strong> contains the normalized coordinates of data point $i$ in the 
                    principal component space</li>
                <li>The <strong>actual projections</strong> of data onto principal components are given by $U \Sigma$ (the 
                    $i$-th row, $j$-th column element is $\sigma_j u_{ij}$, the projection of point $i$ onto component $j$)</li>
            </ul>

            <div class="highlight-box">
                <strong>Summary:</strong> For centered data $\tilde{X} = U \Sigma V^\top$:
                <ul style="margin-top: 0.5rem;">
                    <li>$V$ contains eigenvectors of $\tilde{X}^\top \tilde{X}$ (and of covariance matrix $\Sigma_x$)</li>
                    <li>$U$ contains eigenvectors of $\tilde{X} \tilde{X}^\top$</li>
                    <li>Eigenvalues: $\lambda_i = \frac{\sigma_i^2}{m}$ where $\sigma_i$ are singular values</li>
                </ul>
            </div>

            <div class="highlight-box">
                <strong>When They Meet:</strong> For centered data, PCA and SVD produce the same result. The principal 
                components from PCA (eigenvectors of the covariance matrix) are exactly the right singular vectors 
                from SVD. However, they are still <em>different things</em>‚ÄîPCA is solving a statistical optimization 
                problem, while SVD is performing a matrix factorization. They just happen to coincide for centered data.
            </div>

            <h3>6.1 Why Use SVD for PCA Computationally?</h3>

            <p>
                Even though PCA and SVD are conceptually different, in practice, SVD is often the preferred 
                computational method for PCA because:
            </p>

            <ol>
                <li><strong>Numerical Stability:</strong> SVD algorithms (like those in LAPACK) are more numerically 
                    stable than directly computing eigenvalues of the covariance matrix, especially when $n \gg m$ or $m \gg n$.</li>
                <li><strong>Efficiency:</strong> For large matrices, SVD can be more efficient, especially when we only 
                    need the top $k$ components (truncated SVD).</li>
                <li><strong>Memory Efficiency:</strong> When $n$ is very large, computing $\Sigma_x \in \mathbb{R}^{n \times n}$ 
                    may be infeasible, but SVD can work directly on $\tilde{X}$.</li>
            </ol>

            <p>
                This is why libraries like scikit-learn implement PCA using SVD under the hood‚Äîit's the best 
                computational approach, even though PCA itself is a statistical technique.
            </p>



            <h2>7. Practical Considerations</h2>

            <h3>7.1 Implementation</h3>
            <p>
                In Python, both approaches are available. Here's a complete example showing the relationship:
            </p>

            <div class="code-block">
                <div class="code-header">
                    <span>üêç</span> Python Implementation: PCA vs SVD
                </div>
                <pre><code class="language-python">import numpy as np
from sklearn.decomposition import PCA
from numpy.linalg import svd

# Generate sample data
np.random.seed(42)
m, n = 100, 3  # 100 samples, 3 features
X = np.random.randn(m, n)

# Center the data (crucial for PCA)
X_centered = X - X.mean(axis=0)

# Method 1: PCA using sklearn (uses SVD internally)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_centered)
print("PCA components shape:", X_pca.shape)
print("PCA explained variance:", pca.explained_variance_ratio_)

# Method 2: Direct SVD approach
U, s, Vt = svd(X_centered, full_matrices=False)
# Vt contains the principal components (right singular vectors)
# s contains singular values (related to eigenvalues)
# U contains the projections (left singular vectors)

# Project data using top k components
k = 2
X_svd = U[:, :k] @ np.diag(s[:k])
# Or equivalently:
X_svd_alt = X_centered @ Vt[:k].T

# Verify they give the same result (up to sign)
print("\nAre PCA and SVD results equivalent?")
print("Max difference:", np.abs(X_pca - X_svd_alt).max())

# Eigenvalues from covariance matrix
cov_matrix = (1/(m-1)) * X_centered.T @ X_centered
eigenvals, eigenvecs = np.linalg.eigh(cov_matrix)
eigenvals = eigenvals[::-1]  # Sort descending
eigenvecs = eigenvecs[:, ::-1]

# Relationship: œÉ¬≤ = (n-1) * Œª
print("\nRelationship between singular values and eigenvalues:")
print("Singular values (squared):", s[:k]**2)
print("Eigenvalues (scaled):", eigenvals[:k] * (m-1))
print("They match:", np.allclose(s[:k]**2, eigenvals[:k] * (m-1)))</code></pre>
            </div>

            <p>
                Notice that sklearn's PCA uses SVD internally for numerical stability, but the conceptual 
                framework is still statistical (variance maximization).
            </p>

            <h3>7.2 Computational Complexity</h3>
            <p>
                For a matrix $X \in \mathbb{R}^{n \times d}$:
            </p>
            <ul>
                <li><strong>Full SVD:</strong> $O(\min(n^2d, nd^2))$</li>
                <li><strong>Truncated SVD (top $k$):</strong> $O(knd)$ using iterative methods</li>
                <li><strong>PCA via Covariance:</strong> $O(nd^2 + d^3)$ (computing $C$ + eigendecomposition)</li>
            </ul>

            <p>
                For large-scale problems, truncated SVD is often preferred.
            </p>

     


   


        </div>
    </div>

    <script>
        // Theme toggle functionality
        function toggleTheme() {
            const currentTheme = document.documentElement.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            document.documentElement.setAttribute('data-theme', newTheme);
            localStorage.setItem('theme', newTheme);
        }

        // Load saved theme
        document.addEventListener('DOMContentLoaded', () => {
            const savedTheme = localStorage.getItem('theme') || 'light';
            document.documentElement.setAttribute('data-theme', savedTheme);
        });

        // Update theme icon
        document.addEventListener('DOMContentLoaded', () => {
            const themeToggle = document.querySelector('.theme-toggle');
            const updateIcon = () => {
                const theme = document.documentElement.getAttribute('data-theme');
                themeToggle.querySelector('.theme-icon').textContent = theme === 'dark' ? '‚òÄÔ∏è' : 'üåô';
            };
            updateIcon();
            themeToggle.addEventListener('click', () => {
                setTimeout(updateIcon, 100);
            });
        });

        // Interactive Visualizations
        document.addEventListener('DOMContentLoaded', () => {
            const isDark = document.documentElement.getAttribute('data-theme') === 'dark';
            const bgColor = isDark ? '#1c1917' : '#fafaf9';
            const textColor = isDark ? '#fafaf9' : '#292524';
            const gridColor = isDark ? '#44403c' : '#e7e5e4';
            const primaryColor = isDark ? '#d97706' : '#b45309';
            const secondaryColor = isDark ? '#f59e0b' : '#92400e';

            // // Visualization 1: Effect of Centering with Different Scales
            // if (document.getElementById('centering-viz')) {
            //     // Seeded random number generator
            //     function seededRandom(seed) {
            //         let value = seed;
            //         return function() {
            //             value = (value * 9301 + 49297) % 233280;
            //             return value / 233280;
            //         };
            //     }
                
            //     const rng = seededRandom(42);
            //     const n = 50;
                
            //     // Generate data with very different scales
            //     // X: 0-2 meters (small range)
            //     // Y: 1000-3000 (large range, e.g., weight in grams or price)
            //     const data = [];
            //     for (let i = 0; i < n; i++) {
            //         const x = rng() * 2; // 0 to 2 meters
            //         const y = 1000 + x * 500 + (rng() - 0.5) * 200; // Correlated with x, range ~1000-3000
            //         data.push([x, y]);
            //     }
                
            //     const meanX = data.reduce((s, p) => s + p[0], 0) / n;
            //     const meanY = data.reduce((s, p) => s + p[1], 0) / n;
                
            //     const centeredData = data.map(p => [p[0] - meanX, p[1] - meanY]);
                
            //     // Compute PCA for centered data
            //     const cov = [[0, 0], [0, 0]];
            //     centeredData.forEach(p => {
            //         cov[0][0] += p[0] * p[0];
            //         cov[0][1] += p[0] * p[1];
            //         cov[1][0] += p[1] * p[0];
            //         cov[1][1] += p[1] * p[1];
            //     });
            //     cov[0][0] /= (n - 1);
            //     cov[0][1] /= (n - 1);
            //     cov[1][0] /= (n - 1);
            //     cov[1][1] /= (n - 1);
                
            //     // Simple eigenvalue computation for 2x2
            //     const trace = cov[0][0] + cov[1][1];
            //     const det = cov[0][0] * cov[1][1] - cov[0][1] * cov[1][0];
            //     const lambda1 = trace / 2 + Math.sqrt(trace * trace / 4 - det);
            //     const lambda2 = trace / 2 - Math.sqrt(trace * trace / 4 - det);
                
            //     const pc1 = [cov[0][1], lambda1 - cov[0][0]];
            //     const pc1Norm = Math.sqrt(pc1[0] * pc1[0] + pc1[1] * pc1[1]);
            //     pc1[0] /= pc1Norm;
            //     pc1[1] /= pc1Norm;
                
            //     // For uncentered data, compute PC (this will be biased toward mean)
            //     const dataCov = [[0, 0], [0, 0]];
            //     data.forEach(p => {
            //         dataCov[0][0] += p[0] * p[0];
            //         dataCov[0][1] += p[0] * p[1];
            //         dataCov[1][0] += p[1] * p[0];
            //         dataCov[1][1] += p[1] * p[1];
            //     });
            //     dataCov[0][0] /= (n - 1);
            //     dataCov[0][1] /= (n - 1);
            //     dataCov[1][0] /= (n - 1);
            //     dataCov[1][1] /= (n - 1);
                
            //     const traceUncentered = dataCov[0][0] + dataCov[1][1];
            //     const detUncentered = dataCov[0][0] * dataCov[1][1] - dataCov[0][1] * dataCov[1][0];
            //     const lambda1Uncentered = traceUncentered / 2 + Math.sqrt(traceUncentered * traceUncentered / 4 - detUncentered);
            //     const pc1Uncentered = [dataCov[0][1], lambda1Uncentered - dataCov[0][0]];
            //     const pc1UncenteredNorm = Math.sqrt(pc1Uncentered[0] * pc1Uncentered[0] + pc1Uncentered[1] * pc1Uncentered[1]);
            //     pc1Uncentered[0] /= pc1UncenteredNorm;
            //     pc1Uncentered[1] /= pc1UncenteredNorm;
                
            //     const scale = 1.5;
                
            //     const trace1Uncentered = {
            //         x: data.map(p => p[0]),
            //         y: data.map(p => p[1]),
            //         mode: 'markers',
            //         type: 'scatter',
            //         name: 'Uncentered Data',
            //         marker: { size: 6, color: primaryColor, opacity: 0.7 }
            //     };
                
            //     const trace1Centered = {
            //         x: centeredData.map(p => p[0]),
            //         y: centeredData.map(p => p[1]),
            //         mode: 'markers',
            //         type: 'scatter',
            //         name: 'Centered Data',
            //         marker: { size: 6, color: secondaryColor, opacity: 0.7 },
            //         xaxis: 'x2',
            //         yaxis: 'y2'
            //     };
                
            //     // PC for uncentered (biased)
            //     const pc1TraceUncentered = {
            //         x: [meanX, meanX + pc1Uncentered[0] * scale],
            //         y: [meanY, meanY + pc1Uncentered[1] * scale],
            //         mode: 'lines',
            //         type: 'scatter',
            //         name: 'First PC (biased)',
            //         line: { width: 3, color: '#d53f8c', dash: 'dash' }
            //     };
                
            //     // PC for centered (correct)
            //     const pc1TraceCentered = {
            //         x: [0, pc1[0] * scale],
            //         y: [0, pc1[1] * scale],
            //         mode: 'lines',
            //         type: 'scatter',
            //         name: 'First PC',
            //         line: { width: 3, color: '#d53f8c' },
            //         xaxis: 'x2',
            //         yaxis: 'y2',
            //         showlegend: false
            //     };
                
            //     Plotly.newPlot('centering-viz', 
            //         [trace1Uncentered, pc1TraceUncentered, trace1Centered, pc1TraceCentered], 
            //         {
            //             title: 'Effect of Centering: Different Feature Scales',
            //             xaxis: { 
            //                 title: 'Feature 1 (meters, 0-2)', 
            //                 domain: [0, 0.48], 
            //                 gridcolor: gridColor,
            //                 range: [-0.5, 2.5]
            //             },
            //             yaxis: { 
            //                 title: 'Feature 2 (e.g., weight in grams, 1000-3000)', 
            //                 gridcolor: gridColor,
            //                 range: [500, 3500]
            //             },
            //             xaxis2: { 
            //                 title: 'Feature 1 (centered)', 
            //                 domain: [0.52, 1], 
            //                 gridcolor: gridColor 
            //             },
            //             yaxis2: { 
            //                 title: 'Feature 2 (centered)', 
            //                 anchor: 'x2', 
            //                 gridcolor: gridColor 
            //             },
            //             plot_bgcolor: bgColor,
            //             paper_bgcolor: bgColor,
            //             font: { color: textColor },
            //             showlegend: true,
            //             annotations: [
            //                 {
            //                     x: 0.24,
            //                     y: 0.95,
            //                     xref: 'paper',
            //                     yref: 'paper',
            //                     text: 'Before Centering<br>Different scales dominate',
            //                     showarrow: false,
            //                     font: { size: 12, color: textColor },
            //                     bgcolor: bgColor,
            //                     bordercolor: gridColor,
            //                     borderwidth: 1
            //                 },
            //                 {
            //                     x: 0.76,
            //                     y: 0.95,
            //                     xref: 'paper',
            //                     yref: 'paper',
            //                     text: 'After Centering<br>True variance structure',
            //                     showarrow: false,
            //                     font: { size: 12, color: textColor },
            //                     bgcolor: bgColor,
            //                     bordercolor: gridColor,
            //                     borderwidth: 1
            //                 }
            //             ]
            //         }, 
            //         { responsive: true }
            //     );
            // }

            // Visualization 2: Projection Viewpoint
            if (document.getElementById('projection-viz')) {
                // Seeded random number generator
                function seededRandom2(seed) {
                    let value = seed;
                    return function() {
                        value = (value * 9301 + 49297) % 233280;
                        return value / 233280;
                    };
                }
                
                // Generate 2D data with clear principal direction (elongated ellipse)
                // This creates data with much more variance along one axis
                const rng2 = seededRandom2(123);
                const n2 = 80;
                const angle = Math.PI / 4; // 45 degrees
                const data2 = [];
                
                // Create an elliptical distribution with clear principal direction
                // Major axis: 4 units, Minor axis: 0.8 units
                for (let i = 0; i < n2; i++) {
                    // Generate along major axis (more spread)
                    const t = (rng2() - 0.5) * 4;
                    // Generate along minor axis (less spread)
                    const s = (rng2() - 0.5) * 0.8;
                    // Rotate to create clear principal direction
                    const x = t * Math.cos(angle) - s * Math.sin(angle);
                    const y = t * Math.sin(angle) + s * Math.cos(angle);
                    data2.push([x, y]);
                }
                
                const meanX2 = data2.reduce((s, p) => s + p[0], 0) / n2;
                const meanY2 = data2.reduce((s, p) => s + p[1], 0) / n2;
                const centeredData2 = data2.map(p => [p[0] - meanX2, p[1] - meanY2]);
                
                // Compute PC1
                const cov2 = [[0, 0], [0, 0]];
                centeredData2.forEach(p => {
                    cov2[0][0] += p[0] * p[0];
                    cov2[0][1] += p[0] * p[1];
                    cov2[1][0] += p[1] * p[0];
                    cov2[1][1] += p[1] * p[1];
                });
                cov2[0][0] /= (n2 - 1);
                cov2[0][1] /= (n2 - 1);
                cov2[1][0] /= (n2 - 1);
                cov2[1][1] /= (n2 - 1);
                
                const trace2 = cov2[0][0] + cov2[1][1];
                const det2 = cov2[0][0] * cov2[1][1] - cov2[0][1] * cov2[1][0];
                const lambda1_2 = trace2 / 2 + Math.sqrt(trace2 * trace2 / 4 - det2);
                const pc1_2 = [cov2[0][1], lambda1_2 - cov2[0][0]];
                const pc1Norm2 = Math.sqrt(pc1_2[0] * pc1_2[0] + pc1_2[1] * pc1_2[1]);
                pc1_2[0] /= pc1Norm2;
                pc1_2[1] /= pc1Norm2;
                
                // Projections
                const projections = centeredData2.map(p => {
                    const proj = p[0] * pc1_2[0] + p[1] * pc1_2[1];
                    return [proj * pc1_2[0], proj * pc1_2[1]];
                });
                
                const dataTrace = {
                    x: centeredData2.map(p => p[0]),
                    y: centeredData2.map(p => p[1]),
                    mode: 'markers',
                    type: 'scatter',
                    name: 'Data Points',
                    marker: { size: 6, color: primaryColor, opacity: 0.6 }
                };
                
                // Scale PC line to span the data range
                const dataRange = Math.max(
                    ...centeredData2.map(p => Math.abs(p[0])),
                    ...centeredData2.map(p => Math.abs(p[1]))
                );
                const pcScale = dataRange * 1.2;
                
                const pcTrace = {
                    x: [-pcScale * pc1_2[0], pcScale * pc1_2[0]],
                    y: [-pcScale * pc1_2[1], pcScale * pc1_2[1]],
                    mode: 'lines',
                    type: 'scatter',
                    name: 'Principal Component 1',
                    line: { width: 4, color: primaryColor }
                };
                
                const projTrace = {
                    x: projections.map(p => p[0]),
                    y: projections.map(p => p[1]),
                    mode: 'markers',
                    type: 'scatter',
                    name: 'Projections',
                    marker: { size: 5, color: secondaryColor, symbol: 'diamond' }
                };
                
                // Lines from data to projections
                const lines = [];
                for (let i = 0; i < centeredData2.length; i += 3) {
                    lines.push({
                        x: [centeredData2[i][0], projections[i][0]],
                        y: [centeredData2[i][1], projections[i][1]],
                        mode: 'lines',
                        type: 'scatter',
                        showlegend: i === 0,
                        name: i === 0 ? 'Projection Lines' : '',
                        line: { width: 1, color: '#94a3b8', dash: 'dash', opacity: 0.4 }
                    });
                }
                
                Plotly.newPlot('projection-viz', [dataTrace, pcTrace, projTrace, ...lines], {
                    title: 'PCA Projection Viewpoint',
                    xaxis: { title: 'Feature 1', gridcolor: gridColor },
                    yaxis: { title: 'Feature 2', gridcolor: gridColor, scaleanchor: 'x' },
                    plot_bgcolor: bgColor,
                    paper_bgcolor: bgColor,
                    font: { color: textColor },
                    showlegend: true
                }, { responsive: true });
            }

            // Visualization 3: Scree Plot
            if (document.getElementById('scree-viz')) {
                // Simulated eigenvalues (decreasing)
                const eigenvalues = [8.5, 3.2, 1.8, 0.9, 0.4, 0.2, 0.1, 0.05];
                const totalVar = eigenvalues.reduce((a, b) => a + b, 0);
                const cumVar = [];
                let cumSum = 0;
                eigenvalues.forEach((val, i) => {
                    cumSum += val;
                    cumVar.push(cumSum / totalVar * 100);
                });
                
                const screeTrace = {
                    x: eigenvalues.map((_, i) => i + 1),
                    y: eigenvalues,
                    mode: 'lines+markers',
                    type: 'scatter',
                    name: 'Eigenvalues',
                    marker: { size: 8, color: primaryColor },
                    line: { width: 2, color: primaryColor }
                };
                
                const cumVarTrace = {
                    x: eigenvalues.map((_, i) => i + 1),
                    y: cumVar,
                    mode: 'lines+markers',
                    type: 'scatter',
                    name: 'Cumulative Variance (%)',
                    yaxis: 'y2',
                    marker: { size: 8, color: secondaryColor },
                    line: { width: 2, color: secondaryColor, dash: 'dash' }
                };
                
                Plotly.newPlot('scree-viz', [screeTrace, cumVarTrace], {
                    title: 'Scree Plot: Eigenvalues and Cumulative Variance',
                    xaxis: { title: 'Component Number', gridcolor: gridColor },
                    yaxis: { 
                        title: 'Eigenvalue', 
                        gridcolor: gridColor,
                        side: 'left'
                    },
                    yaxis2: {
                        title: 'Cumulative Variance (%)',
                        overlaying: 'y',
                        side: 'right',
                        gridcolor: gridColor
                    },
                    plot_bgcolor: bgColor,
                    paper_bgcolor: bgColor,
                    font: { color: textColor },
                    showlegend: true,
                    legend: { x: 0.7, y: 0.3 }
                }, { responsive: true });
            }
        });
    </script>
</body>
</html>
